{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from merge.preprocessing.tokenizer import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.create(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.create(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"Hello, my dog is cute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = load_dataset(\"roneneldan/TinyStories\", split=\"train\", cache_dir=\"./hf_cache\", num_proc=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from merge.preprocessing.tokenizer import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.create(\"EleutherAI/gpt-neo-125m\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers.processors as processors\n",
    "\n",
    "bos = \"<|begin_of_text|>\"\n",
    "tokenizer._tokenizer.post_processor = processors.Sequence(\n",
    "    [\n",
    "        processors.ByteLevel(trim_offsets=False),\n",
    "        processors.TemplateProcessing(\n",
    "            single=f\"{bos}:0 $A:0\",\n",
    "            pair=f\"{bos}:0 $A:0 {bos}:1 $B:1\",\n",
    "            special_tokens=[\n",
    "                (bos, tokenizer.bos_token_id),\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[50256, 1212, 318, 257, 1332, 11], [50256, 1212, 318, 50257, 50257, 50257]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 0, 0, 0]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [\"This is a test, what is a test\", \"This is\"]\n",
    "\n",
    "tokenizer(batch, truncation=True, padding=\"max_length\", max_length=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(50257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonericsson/projects/github/merge/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from merge.preprocessing.tokenizer import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.create(\"microsoft/phi-2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
    "\n",
    "tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "fw = load_from_disk(\"data/pretokenized_roneneldan_TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from merge.modules.attention import causal_attention_mask\n",
    "\n",
    "mask = causal_attention_mask(512)\n",
    "\n",
    "type(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from merge.modules.attention import causal_attention_mask\n",
    "from merge.architectures.llama import Llama, LlamaConfig\n",
    "\n",
    "model_config = LlamaConfig(\n",
    "    vocab_size=50257,\n",
    "    seq_len=512,\n",
    "    d_model=256,\n",
    "    hidden_dim=512,\n",
    "    num_heads=8,\n",
    "    num_kv_heads=4,\n",
    "    num_layers=6,\n",
    "    rope_theta=0.1,\n",
    "    norm_eps=1e-6,\n",
    "    activation_fn=\"silu\",\n",
    "    mlp_bias=False,\n",
    "    mlp_dropout=0.0,\n",
    "    attention_bias=False,\n",
    "    attention_dropout=0.0,\n",
    "    pos_encoding_type=\"rope\",\n",
    "    mlp=\"mlp_swiglu\",\n",
    "    normalization=\"rmsnorm\",\n",
    "    attention=\"gqa\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causal mask: None\n"
     ]
    }
   ],
   "source": [
    "model = Llama(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0000e+00, -1.0000e+04, -1.0000e+04,  ..., -1.0000e+04,\n",
       "           -1.0000e+04, -1.0000e+04],\n",
       "          [ 1.0000e+00,  1.0000e+00, -1.0000e+04,  ..., -1.0000e+04,\n",
       "           -1.0000e+04, -1.0000e+04],\n",
       "          [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  ..., -1.0000e+04,\n",
       "           -1.0000e+04, -1.0000e+04],\n",
       "          ...,\n",
       "          [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  ...,  1.0000e+00,\n",
       "           -1.0000e+04, -1.0000e+04],\n",
       "          [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  ...,  1.0000e+00,\n",
       "            1.0000e+00, -1.0000e+04],\n",
       "          [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  ...,  1.0000e+00,\n",
       "            1.0000e+00,  1.0000e+00]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from merge.modules.transformer import Transformer\n",
    "\n",
    "model_t = Transformer(model_config)\n",
    "\n",
    "model_t.causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Let's simulate a vocabulary where:\n",
    "# pad_token_id = 0\n",
    "# a_token_id = 1\n",
    "# b_token_id = 2\n",
    "# vocab_size = 3\n",
    "\n",
    "# Original sequence: \"aab\" padded to length 5\n",
    "original = torch.tensor([[1, 1, 2, 0, 0, 0]])  # [a, a, b, pad, pad]\n",
    "input_ids = original[:, :-1]  # [a, a, b, pad, pad]\n",
    "target_ids = original[:, 1:] # [a, b, pad, pad, pad]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 1, 2, 0, 0]]), tensor([[1, 2, 0, 0, 0]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fake logits - we'll make them \"perfect\" predictions\n",
    "# Shape: (batch=1, seq=5, vocab=3)\n",
    "logits = torch.zeros(1, 5, 3)\n",
    "# For each position, make the correct prediction have highest probability\n",
    "logits[0, 0, 1] = 10.0  # predict 'a' at position 0\n",
    "logits[0, 1, 2] = 10.0  # predict 'b' at position 1\n",
    "# We don't care about the predictions for padded positions\n",
    "\n",
    "# Test both with and without ignore_index\n",
    "criterion_with_ignore = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "criterion_without_ignore = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with ignore_index=0: 9.083335316972807e-05\n",
      "Loss without ignore_index: 0.6592037081718445\n"
     ]
    }
   ],
   "source": [
    "# Reshape for loss calculation\n",
    "batch, seq, vocab = logits.shape\n",
    "logits_reshaped = logits.reshape(batch * seq, vocab)\n",
    "target_ids_reshaped = target_ids.reshape(batch * seq)\n",
    "\n",
    "loss_with_ignore = criterion_with_ignore(logits_reshaped, target_ids_reshaped)\n",
    "loss_without_ignore = criterion_without_ignore(logits_reshaped, target_ids_reshaped)\n",
    "\n",
    "print(f\"Loss with ignore_index=0: {loss_with_ignore.item()}\")\n",
    "print(f\"Loss without ignore_index: {loss_without_ignore.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3]), torch.Size([5]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_reshaped.shape, target_ids_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.0833e-05)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_with_ignore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
