{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from omni.preprocessing.tokenizer import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.create(\"EleutherAI/gpt-neo-125m\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omni.modules.pos_embeddings import precompute_freqs_cis_real\n",
    "\n",
    "pos_embeddings = precompute_freqs_cis_real(64, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = torch.pow(2.0, torch.tensor(-8/16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_alibi_bias(\n",
    "    num_heads, seq_length\n",
    "):\n",
    "    ratio = 2 ** (-8 / num_heads)\n",
    "    slopes = torch.pow(ratio, torch.arange(1, num_heads + 1, dtype=torch.float32))\n",
    "    positions = -torch.arange(seq_length, dtype=torch.float32)\n",
    "    return slopes[:, None] * positions[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "alibi = precompute_alibi_bias(8, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alibi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "qk = torch.randn(64, 8, 512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 8, 512, 512])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(qk + alibi[None, :, None, :]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00390625"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 / 2**8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.+0.j)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = precompute_freqs_cis(64, 512)\n",
    "freq[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Weights:\n",
      " Parameter containing:\n",
      "tensor([[ 0.6366, -0.6068,  1.0638, -0.2998, -0.3673],\n",
      "        [ 0.5253, -0.5211, -1.5515, -0.3167,  1.1084],\n",
      "        [-0.1319, -1.7915, -1.8240,  2.5206,  0.6195],\n",
      "        [-1.0323,  0.6575,  1.0189, -1.6698, -1.6149],\n",
      "        [ 0.7588,  1.7124, -0.3617,  0.1542, -2.0302],\n",
      "        [ 0.5312, -1.1708, -0.7935,  3.2636, -0.4199],\n",
      "        [ 1.0472, -0.5237,  1.2043, -0.3793,  1.6037],\n",
      "        [ 0.6935, -0.3820, -0.0632,  0.0515,  0.5252],\n",
      "        [-0.8518,  0.3274, -1.0181,  0.5778, -1.4131],\n",
      "        [-0.3643,  0.2370, -0.3215, -0.6997,  1.3129]], requires_grad=True)\n",
      "Shape of Weights: torch.Size([10, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a dummy nn.Embedding layer\n",
    "embedding_layer = nn.Embedding(num_embeddings=10, embedding_dim=5)  # 10 positions, 5 dimensions each\n",
    "\n",
    "# Inspect the weights\n",
    "weights = embedding_layer.weight\n",
    "print(\"Embedding Weights:\\n\", weights)\n",
    "print(\"Shape of Weights:\", weights.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Embedding' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43membedding_layer\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Embedding' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "embedding_layer[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omni.utils.tools import auto_device\n",
    "\n",
    "\n",
    "device = auto_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.amp.autocast_mode.is_autocast_available(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.amp.GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.0833e-05)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_with_ignore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
